\chapter{Reinforcement Learning}

\section{The RL Problem Setting}

TODO: image of agent environment interaction.

\subsection{The Basics}

Reinforcement learning involves an \textbf{agent} interacting within an \textbf{environment}. The agent selects actions, and the environment responds with new states and rewards. Sutton \& Barto \cite{sutton1998reinforcement} identify four additional sub-elements to reinforcement learning systems:
\begin{itemize}
	\item a \textbf{policy},
	\item a \textbf{reward signal},
	\item a \textbf{value function},
	\item and, optionally, a \textbf{model} of the environment.
\end{itemize}
\noindent A \textit{policy} defines a mapping from (perceived) states to output actions to be taken. A \textit{reward signal} is a real-valued number that the agent receives from the environment. The agent tries to maximize its total reward, which is referred to as value. The \textit{value function} specifies the expected value, starting from a particular state. Thus, it is ultimately value that we try to optimize. Finally, a \textit{model} of the environment is the agent's simulation of the environment, allowing the agent to plan actions and predict outcomes (states, rewards, etc.).

The boundary between agent and environment is often not clearly defined. Sutton \& Barto place the agent-environment boundary at the limit of the agent's control, not at the physical boundary. For instance, the limbs of the agent, its internal energy reserves, and even the reward mechanism are considered to be part of the environment, since the agent does not have absolute control over these aspects.

We assume that time unfolds in a series of discrete time-steps, $t = 1, 2, 3, \dots$ At each time step, the agent is in some state $S_t \in \mathcal{S}$ and chooses some action $A_t \in \mathcal{A} (S_t)$. Here, $\mathcal{S}$ denotes the space of possible states and $\mathcal{A} (S_t)$ denotes the space of possible actions from state $S_t$. The action is chosen based on the agent's policy $\pi_t (a | s)$, which probabilistically maps states $S_t = s$ to actions $A_t = a$. At the next time step, the agent receives a real-valued reward, $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$, and arrives in the next state, $S_{t+1}$.

The \textbf{return}, $G_t$, is the (discounted) sum of rewards starting at time $t$. For \textbf{episodic tasks}, in which the sequence has a finite length, $T$, this is defined as 

\begin{equation}
G_t \triangleq R_{t+1} + R_{t+2} + \dots + R_T = \sum_{k=0}^{T - t - 1} R_{t + k + 1},
\end{equation}

\noindent However, for \textbf{continuing tasks}, in which the time sequence length can be infinite, we include a \textbf{discount rate}, $\gamma \in [0, 1]$, to prevent the return from going to infinity:

\begin{equation}
G_t \triangleq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t + k + 1}.
\end{equation}

The \textbf{value function} or \textbf{state-value function}, $v_\pi (s)$, is defined as the expected return from being in state $s$ when using policy $\pi$:

\begin{equation}
	v_\pi (s) \triangleq \mathbb{E}_\pi \left[ G_t | S_t = s \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t + k + 1} | S_t = s \right].
	\label{eq: value_func_def}
\end{equation}

\noindent Note that here we are assuming that the task respects the Markov property; the current state, along with the policy, contains all of the information to determine the value function. We can also define an \textbf{action-value function}, $q_\pi (s, a)$, which specifies expected returns for each action $a$ taken from state $s$ using policy $\pi$:

\begin{equation}
	q_\pi (s, a) \triangleq \mathbb{E}_\pi \left[ G_t | S_t = s, A_t = a \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t + k + 1} | S_t = s, A_t = a \right].
\end{equation}

Using the definition of the value function, we can derive a recursive relationship known as the \textbf{Bellman equation}. Starting from eq. \ref{eq: value_func_def}:

\begin{align}
	\label{eq: bellman_0}
	v_\pi (s) &\triangleq \mathbb{E}_\pi \left[ G_t | S_t = s \right] \\
	\label{eq: bellman_1}
	&= \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t + k + 1} \bigg\vert S_t = s \right] \\
	\label{eq: bellman_2}
	&= \mathbb{E}_\pi \left[ R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^k R_{t + k + 2} \bigg\vert S_t = s \right] \\
	\label{eq: bellman_3}
	&= \sum_{a, s^\prime, r} \pi(a | s) p(s^\prime, r | s, a) \left[ r + \gamma \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t + k + 2} \bigg\vert S_{t+1} = s^\prime \right] \right] \\
	\label{eq: bellman_4}
	&= \sum_{a, s^\prime, r} \pi(a | s) p(s^\prime, r | s, a) \left[ r + \gamma v_\pi (s^\prime) \right].
\end{align}

\noindent To go from eq. \ref{eq: bellman_1} to eq. \ref{eq: bellman_2}, we split the first term out of the sum. To arrive at \ref{eq: bellman_3}, we move the expectation to the future summation, re-expressing the expectation over the first step using summations to weight each return by the appropriate probability of the action $a$, state $s^\prime$, and reward $r$. Finally, to arrive at  \ref{eq: bellman_4}, we notice that the expectation is the value of state $s^\prime$ under our policy, i.e. $v_\pi (s^\prime)$. Thus, we see that the value function has the recursive definition:

\begin{equation}
	v_\pi (s) = \sum_{a, s^\prime, r} \pi(a | s) p(s^\prime, r | s, a) \left[ r + \gamma v_\pi (s^\prime) \right].
\end{equation}

\noindent A similar recursive definition can be derived for $q_\pi (s, a)$.

The \textbf{optimal value function} $v_* (s)$

\subsection{Summary of Notation}


\section{Value Based Methods}

\subsection{Multi-Armed Bandits}

The multi-armed bandit problem is an example of a \textit{non-associative setting}, in which the agent only learns to act in one situation. This simplifies much of the reinforcement learning problem. In particular, the \textbf{multi-armed bandit} problem involves a situation with $k$ possible actions (sometimes referred to as arms, in analogy to slot machines), each with an associated value. The value of action $a$ is denoted as $q_* (a)$, the \textbf{action-value function}. The interaction between the agent and the environment consists of a series of episodes, each of duration 1 time step. At time step $t$, the agent takes \textbf{action} $A_t$ and receives \textbf{reward} $R_t$. Thus, for some action $A_t = a$, the action-value function is defined as

\begin{equation}
	q_* (a) \triangleq \mathbb{E} \left[ R_t | A_t = a \right].
\end{equation}

\noindent Note that rewards can be a stochastic function of the action chosen. $q_* (a)$ encodes the \textit{mean} reward of choosing action $a$. In multi-armed bandit problems, the action-value function is initially unknown. To optimize total reward, we must estimate the action-value function to find the action or actions with the highest reward. Selecting this action will then optimize reward. We refer to the \textbf{action-value estimates} at time step $t$ as $Q_t (a)$.

At any time step $t$, we can either \textbf{exploit} our current knowledge by choosing the action with the optimal action-value estimate, or we can \textbf{explore} other actions with non-optimal action-value estimates. Exploitation is a greedy process, which will likely yield higher rewards in the short-term. Exploration, on the other hand, may find better actions, which can later be exploited, allowing it to likely yield higher rewards in the long-term.

We can select actions according to different strategies. For instance, we could select the action with the optimal action-value estimate at each time step:

\begin{equation}
A_t = \text{argmax}_a Q_t (a)
\end{equation}

\noindent However, this policy will never explore other actions, and may therefore perform poorly in the long run. Another strategy is to select the action with the optimal action-value estimate with probability $1 - \epsilon$ and select a random action with probability $\epsilon$. This strategy is referred to as $\epsilon$\textbf{-greedy}.

\subsection{Dynamic Programming Methods}

\subsubsection{Policy Iteration}

\subsubsection{Value Iteration}

\subsection{Monte Carlo Methods}

\subsection{TD Learning}

\subsubsection{Q Learning}

\subsubsection{SARSA}



\section{Policy Gradient Methods}

\subsection{REINFORCE}

\subsection{Actor-Critic Methods}


\section{Intrinsic Motivation}

Intrinsic motivation is to reinforcement learning as unsupervised learning is to supervised learning. By learning from general purpose signals that connect the perception-action loop, intrinsic motivation can allow an agent to learn how to perceive and act even in the presence of a sparse or non-existent reward signal. This can dramatically speed up learning when an environmental reward signal is encountered. In \cite{oudeyer2008can}, the authors characterize intrinsic motivation using the concept of \textit{collative variables} \cite{berlyne1965structure}, which refer to various measures of subjective uncertainty, such as novelty, surprise, ambiguity, complexity, etc., all of which can be expressed in terms of information theoretic quantities. The computational landscape described in \cite{oudeyer2008can} consists of:
\begin{enumerate}
	\item \textbf{Knowledge based models} involve comparisons between predicted sensorimotor states from an internal model and the actual states experienced
	\item \textbf{Competence based models} involve comparisons between self-generated goals and the extent to which they are actually reached
	\item \textbf{Morphological models} involve finding relationships between multiple sensorimotor channels which are not based on the agent's long-term knowledge or competence
\end{enumerate}
A majority of intrinsic motivation approaches fall into the first category, which is the focus of this section. This includes measures such as Bayesian surprise \cite{itti2006bayesian}, curiosity \cite{schmidhuber2010formal}, prediction errors \cite{nelson2005finding, barto2004intrinsically}, and information theoretic measures \cite{klyubin2005empowerment, little2013learning, wissner2013causal}. 

\subsection{Empowerment}

We can consider an agent's perception-action loop as a noisy communication channel: an agent performs actions, which can affect the environment and transmit information to the agent's sensory states. Ideally, an agent should be able to control and predict aspects of its environment, effectively communicating with itself through its own actions. \textbf{Empowerment} formalizes this notion, corresponding to the maximum mutual information (i.e. \textit{channel capacity}) between an agent's actions and its sensory states \cite{klyubin2005empowerment}. Denoting a sequence of $n$ actions starting at time step $t$ as $a^n_t \in A^n_t$ and the state at time $t$ as $S_t = s_t$, empowerment is expressed as
\begin{equation}
    \mathcal{E} (s_t) \equiv \max_{p (a^n_t | s_t)} I(A^n_t; S_{t+n}).
    \label{eq: empowerment def}
\end{equation}
\noindent That is, given the distribution of all possible  $n$-step action sequences from the current state, empowerment is the maximum mutual information between an action sequence and the agent's corresponding sensory state $n$ time steps later. The conditional distribution $p (a^n_t | s_t)$ is the agent's policy. From the definition of mutual information, we can also write this as:
\begin{equation}
    \mathcal{E} (s_t) = \max_{p (a^n_t | s_t)} \mathbb{E}_{p(a^n_t , s_{t+n} | s_t)} \left[ \log \left( \frac{p(a^n_t , s_{t+n} | s_t)}{p (a^n_t | s_t) p (s_{t+n} | s_t)} \right) \right]
    \label{eq: empowerment def 2}
\end{equation}
Note that empowerment is only concerned with the \textit{capacity} to affect one's own sensory states, not the actual action sequence performed. 

From eqs. \ref{eq: empowerment def} and \ref{eq: empowerment def 2}, there are three methods by which an agent can increase empowerment:
\begin{enumerate}
    \item attempt to reach states where $\mathcal{E} (s_t)$ is high, i.e. where $p(s_{t+n} | a^n_t, s_t)$ is less noisy,
    \item modify one's sensors or perception model to improve empowerment, i.e. forming a better state representation where different environmental states can be discriminated,
    \item modify one's actuators or policy model to improve empowerment, i.e. taking better actions that result in specific environmental states.
\end{enumerate}
\noindent The first method operates over shorter time horizons and is similar to \textit{inference}, in which an agent changes its current states (and/or actions). The second and third methods operate over longer time horizons and are similar to \textit{learning}, in which an agent changes its model parameters.

\subsubsection{Variational Lower Bound on Empowerment}
Rewriting $p(a^n_t , s_{t+n} | s_t) = p(s_{t+n} | a^n_t, s_t) p( a^n_t | s_t)$, we see that the expectation in eq. \ref{eq: empowerment def 2} involves marginalizing over the environment dynamics $p(s_{t+n} | a^n_t , s_t)$. Even if this dynamics model were known, computing the mutual information would be intractable for long time horizons $n$ or complex environments with many states and actions. Instead, we can use a variational distribution over actions, $q(a^n_t | s_{t+n}, s_t)$, to induce a lower bound on the mutual information, then maximize this lower bound \cite{barber2003algorithm}. The mutual information can be rewritten as
\begin{equation}
    I(A^n_t ; S_{t+n}) = H(A^n_t) - H(A^n_t | S_{t+n}).
\end{equation}
The conditional entropy is bounded by
\begin{equation}
    H(A^n_t | S_{t+n}) \leq - \mathbb{E}_{p(a^n_t , s_{t+n} | s_t)} \left[ \log q(a^n_t | s_{t+n}, s_t) \right].
\end{equation}
Therefore, the mutual information is bounded by
\begin{equation}
    I(A^n_t ; S_{t+n}) \geq H(A^n_t) + \mathbb{E}_{p(a^n_t , s_{t+n} | s_t)} \left[ \log q(a^n_t | s_{t+n}, s_t) \right],
\end{equation}
\begin{equation}
    I(A^n_t ; S_{t+n}) \geq H(A^n_t) + \mathbb{E}_{p(s_{t+n} | a^n_t , s_t) p(a^n_t | s_t)} \left[ \log q(a^n_t | s_{t+n}, s_t) \right].
\end{equation}
The set-up has an intuitive interpretation \cite{barber2003algorithm, mohamed2015variational}. The marginal action sequence distribution $p(a^n_t | s_t)$ can be regarded as a \textit{source} or \textit{exploration} distribution over which we draw actions, i.e. the policy. Upon selecting an action sequence, the distribution $p(s_{t+n} | a^n_t , s_t)$ can be regarded as an \textit{encoder} or \textit{transition} distribution that maps an action sequence to a future state. The variational distribution $q(a^n_t | s_{t+n}, s_t)$ then acts as a \textit{decoder} or \textit{planning} distribution, mapping some future state back to an action sequence.

Having derived a lower bound on the mutual information, empowerment is approximately calculated by maximizing over the parameters of the distributions $p(a^n_t | s_t)$ and $q(a^n_t | s_{t+n}, s_t)$:
\begin{equation}
    \mathcal{E} (s_t) = \max_{p (a^n_t | s_t), q(a^n_t | s_{t+n}, s_t)} \left( H(A^n_t) + \mathbb{E}_{p(a^n_t , s_{t+n} | s_t)} \left[ \log q(a^n_t | s_{t+n}, s_t) \right] \right).
    \label{eq: IM equation}
\end{equation}
When performed using alternating optimization, this is referred to as the information maximization (IM) algorithm \cite{barber2003algorithm}. It is noted in \cite{mohamed2015variational} that eq. \ref{eq: IM equation} can be difficult to optimize. The authors address this by placing a constraint on $H(A^n_t)$, essentially re-weighting the entropy term.

\subsection{Uncertainty Motivation}

\subsection{Information Gain Motivation}

\subsection{Predictive Novelty Motivation}

Predictive novelty motivation assigns an intrinsically motivated reward that is proportional to the agent's prediction error on an observed sensorimotor state. This can be expressed in probabilistic or non-probabilistic terms. The reward of the novel sensorimotor state will drive the agent to repeatedly explore that state, while at the same time, the agent's internal model improves to minimize prediction error, thereby reducing the novelty of the state. This approach, along with the option framework, is used in \cite{barto2004intrinsically} to learn a hierarchical collection of skills, allowing the agent to more effectively optimize a sparse extrinsic reward signal. It should also be noted that one must be careful in assigning rewards to prediction errors, as extreme novelty (i.e. a random, unpredictable environment) should have an aversive effect on the agent.

\subsection{Learning Progress Motivation}

\noindent \cite{gregor2016variational} introduce variational intrinsic control within the option framework.

