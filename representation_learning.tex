\chapter{Representation Learning}

\section{Motivation \& Definitions}

\textit{Representation learning} is the process of taking data $\mathbf{x}$, or some collection thereof, and forming a representation $\mathbf{z}$, that captures some meaningful aspects of the data. Note that, due to the data processing inequality, there can be no creation of additional information, only the transformation of the information contained in $\mathbf{x}$.

A fundamental aspect of representation learning is that it is inherently ill-defined. There is no way in which to assess the quality of a representation, i.e. which aspects of the data to capture, without defining a \textit{task} which one hopes to accomplish with the representation. We will refer to the output of this task as $\mathbf{y}$. Given this task, one can then define an \textit{ideal representation} for that task. \cite{achille2017emergence} provides a theory, based on the Information Bottleneck Lagrangian \cite{tishby2000information}, defined as

\begin{equation}
	\mathcal{L}(p(\mathbf{z} | \mathbf{x})) \triangleq H(\mathbf{y} | \mathbf{z}) + \beta I(\mathbf{z} ; \mathbf{x}),
\end{equation}

\noindent where $\beta$ is a regularization weight, for forming such representations. They define an ideal representation $\mathbf{z}$ as one that is:

\begin{enumerate}
	\item \textbf{sufficient}: capturing the information contained in $\mathbf{x}$ that is sufficient to estimate $\mathbf{y}$, i.e. $I(\mathbf{y}; \mathbf{z}) = I(\mathbf{y}; \mathbf{x})$.
	\item \textbf{minimal}: retains as little information about $\mathbf{x}$ as possible, making the task easier, i.e. $I(\mathbf{z}; \mathbf{x})$ is minimized.
	\item \textbf{invariant}: nuisances $n$ have no effect on the representation, which helps to prevent overfitting i.e. $I(\mathbf{z}; n) = 0$.
	\item \textbf{disentangled}: the total correlation, defined as the dissimilarity between the representation and the product of its marginals, $TC(\mathbf{z}) \triangleq D_{KL} (p(\mathbf{z}) || p(\mathbf{z}_1) p(\mathbf{z}_2) \dots p(\mathbf{z}_L))$, is minimized.
\end{enumerate}

\noindent \cite{achille2017emergence} show that only (1) and (2) need to be enforced, as (3) and (4) naturally follow.


\cite{bengio2013representation} has a good review of different desirable qualities of representations.


\section{Representation Learning Without a Task}

Often, we do not have a well-defined task, however, we would still like to learn a representation that captures aspects of the data that will facilitate rapid transfer learning to future tasks. This is referred to as \textit{unsupervised learning}. The lack of task implies that we no longer have a measure of sufficiency, meaning that we must define an alternative quantity to optimize. 

\section{Disentangled Representations}

Define disentanglement. Why are disentangled representations helpful? Trade off between disentanglement and faithfully representing the data. How do we learn disentangled representations, while at the same time, respecting the structure of the latent space? Importance of priors.

Two random variables are \textbf{independent} if their joint probability can be expressed as the product of their marginals:

\begin{equation}
	p(\mathbf{x}, \mathbf{y}) = p(\mathbf{x}) p(\mathbf{y}),
\end{equation}

\noindent and they are \textbf{conditionally independent} if their conditional joint probability can be expressed as

\begin{equation}
	p(\mathbf{x}, \mathbf{y} | \mathbf{z}) = p(\mathbf{x} | \mathbf{z}) p(\mathbf{y} | \mathbf{z}).
\end{equation}

Independence is related to covariance, but is a stronger property. Two variables that are independent have zero covariance and two variables that have non-zero covariance are dependent. Zero covariance implies that the variables have no \textit{linear} dependence. Independence implies that the variables also have no \textit{non-linear} dependence. That is, it is possible for two dependent variables to have zero covariance. 
\\
\\
\noindent \textbf{Notes:}

\cite{cheung2014discovering} propose using a cross-covariance penalty term in the setting of semi-supervised learning of the form:

\begin{equation}
	C(\hat{\mathbf{y}}^{1 \dots N}, \mathbf{z}^{1 \dots N}) = \frac{1}{2} \sum_{ij} \left[ \frac{1}{N} \sum_n (\hat{y}^n_i - \bar{\hat{y}}_i) (z^n_j - \bar{z}_j) \right]^2,
\end{equation}

\noindent where $\hat{\mathbf{y}}$ is a vector of (one-hot) inferred labels and $\mathbf{z}$ is a latent representation. $N$ is the batch size.


\cite{higgins2016early} propose to use a regularization weight in a VAE setting to ``upweight" the amount of regularization as a means of promoting \textbf{redundancy reduction and independence} among the latent representation. They also argue for the importance of \textbf{dense sampling of the (continuous) data manifold} for disentanglement. Sparse sampling of the data manifold results in ambiguity is the manifold interpretation, requiring additional supervision for disentanglement.

\cite{siddharth2016inducing} use supervision to impose structure on part of the latent representation in a VAE.

Dropout \cite{srivastava2014dropout} is a technique for preventing ``fragile coadaptation" between units within a representation, effectively enforcing that they represent different (independent) quantities. This technique also results in redundancy within the representation.