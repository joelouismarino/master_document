\chapter{Variational Inference}
\label{chap: variational inference}

\section{Inference as Optimization: Deriving the ELBO}

Consider a model, $p(\mathbf{x}, \mathbf{z})$, that specifies a joint distribution over a latent variable $\mathbf{z}$ and observed variable $\mathbf{x}$. To infer the posterior distribution of $\mathbf{z}$ given $\mathbf{x}$, we can use the definition of conditional probability:

\begin{equation}
	p (\mathbf{z} | \mathbf{x}) = \frac{p(\mathbf{x}, \mathbf{z})}{p (\mathbf{x})},
	\label{eq: bayes rule}
\end{equation}

\noindent where the marginal likelihood (a.k.a. model evidence or partition function) $p(\mathbf{x})$ is calculated by marginalizing over the latent state space:

\begin{equation}
	p (\mathbf{x}) = \int p(\mathbf{x}, \mathbf{z}) d\mathbf{z}.
	\label{eq: marginalize}
\end{equation}

 \noindent For large latent spaces and/or complicated models, performing the integration in eq. \ref{eq: marginalize} is computationally intractable.
 
 Variational inference transforms this intractable integration problem into an \textit{optimization} problem by introducing an approximate posterior distribution, $q (\mathbf{z} | \mathbf{x})$, typically chosen from some simple family of distributions, such as independent Gaussians. We attempt to make $q (\mathbf{z} | \mathbf{x})$ as ``close" as possible to $p (\mathbf{z} | \mathbf{x})$ by minimizing $ D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z} | \mathbf{x}))$. Notice that the word close is in quotations because KL divergence is not a true distance measure; it is not symmetric. When the (non-negative) KL divergence between these distributions is zero, we recover the true posterior, $p (\mathbf{z} | \mathbf{x})$. We cannot evaluate $ D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z} | \mathbf{x}))$ directly, as it includes the true posterior, which we cannot tractably compute. Instead, we will maximize a lower bound on $p(\mathbf{x})$, which will have the effect of minimizing $ D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z} | \mathbf{x}))$, which we will now show. We start from the definition of KL divergence:
 
 \begin{equation}
 	D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z} | \mathbf{x})) = \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z} | \mathbf{x})} \left[ \log \frac{q (\mathbf{z} | \mathbf{x})}{p (\mathbf{z} | \mathbf{x})} \right],
	\label{eq: elbo derivation 1}
 \end{equation}
 
 
 \begin{equation}
 	D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z} | \mathbf{x})) = \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z} | \mathbf{x})} \left[ \log q (\mathbf{z} | \mathbf{x}) \right] - \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z} | \mathbf{x})} \left[ \log p (\mathbf{z} | \mathbf{x}) \right].
	\label{eq: elbo derivation 2}
 \end{equation}
 
 \noindent Plugging in the definition of conditional probability into the second term yields:
 
  \begin{equation}
 	D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z} | \mathbf{x})) = \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z} | \mathbf{x})} \left[ \log q (\mathbf{z} | \mathbf{x}) \right] - \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z} | \mathbf{x})} \left[ \log \frac{p (\mathbf{x}, \mathbf{z})}{p (\mathbf{x})} \right],
	\label{eq: elbo derivation 3}
 \end{equation}
 
 \noindent which can be expanded into
 \begin{equation}
 	D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z} | \mathbf{x})) = \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z} | \mathbf{x})} \left[ \log q (\mathbf{z} | \mathbf{x}) \right] - \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z} | \mathbf{x})} \left[ \log p (\mathbf{x}, \mathbf{z}) \right] + \log p (\mathbf{x}).
	\label{eq: elbo derivation 4}
 \end{equation}
 
 \noindent Now, we'll define the following quantity, which we refer to as the \textit{evidence lower bound (ELBO)} or \textit{variational lower bound} or \textit{negative free energy}:
 
\begin{equation}
 	\boxed{\mathcal{L} \equiv \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z} | \mathbf{x})} \left[ \log p (\mathbf{x}, \mathbf{z}) - \log q (\mathbf{z} | \mathbf{x}) \right]}
	\label{eq: elbo derivation 5}
\end{equation}

\noindent Plugging this definition back into eq. \ref{eq: elbo derivation 4}, we get

   \begin{equation}
 	D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z} | \mathbf{x})) = \log p (\mathbf{x})  - \mathcal{L}.
	\label{eq: elbo derivation 6}
 \end{equation}
 
 \noindent Rearranging terms, we see
 
    \begin{equation}
 	\log p (\mathbf{x})  =  \mathcal{L} + D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z} | \mathbf{x})).
	\label{eq: elbo derivation 7}
 \end{equation}

\noindent Since $D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z} | \mathbf{x}))$ is non-negative, $\mathcal{L}$ is a lower bound on $\log p (\mathbf{x})$. Further, since $\log p (\mathbf{x})$ is not dependent on $q (\mathbf{z} | \mathbf{x})$, we see that maximizing $\mathcal{L}$ with respect to $q (\mathbf{z} | \mathbf{x})$ must minimize $D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z} | \mathbf{x}))$. Thus, we can approximate the true posterior by optimizing $\mathcal{L}$ with respect to $q (\mathbf{z} | \mathbf{x})$.

Note that the ELBO can also be written in the following form:

\begin{equation}
 	\mathcal{L} = \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z} | \mathbf{x})} \left[ \log p (\mathbf{x}, \mathbf{z}) - \log q (\mathbf{z} | \mathbf{x}) \right]
	\label{eq: elbo derivation 8}
 \end{equation}
 
\begin{equation}
 	\mathcal{L} = \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z} | \mathbf{x})} \left[ \log p (\mathbf{x} | \mathbf{z})  + \log p(\mathbf{z}) - \log q (\mathbf{z} | \mathbf{x}) \right]
	\label{eq: elbo derivation 9}
 \end{equation}
 
\begin{equation}
 	\mathcal{L} = \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z} | \mathbf{x})} \left[ \log p (\mathbf{x} | \mathbf{z}) \right] - D_{KL}(q (\mathbf{z} | \mathbf{x}) || p (\mathbf{z}))
	\label{eq: elbo derivation 10}
 \end{equation}

\noindent This highlights that the ELBO specifies the optimal $q (\mathbf{z} | \mathbf{x})$ by trading off between representing the input, through the first term, and agreeing with the prior on the latent variables, through the second term. In other words, the first term attempts to fit the data, while the second term regularizes the representation.



\section{Normalizing Flows} 

A common drawback of variational inference is that it is typically restricted to families of approximate posterior densities, e.g. factorized Gaussians. \textit{Normalizing flows} \cite{rezende2015variational} is a method for fitting flexible approximate posterior densities. It uses a sequence of invertible mappings, called a `flow,' to transform simple posterior densities into arbitrary, flexible forms. If we start with a variable $\mathbf{z}$, drawn from a distribution $q(\mathbf{z})$, and apply an invertible, smooth mapping $f(\mathbf{z})$, then the change of variables formula allows us to write

\begin{equation}
q(\mathbf{z}^\prime) = q(\mathbf{z}) \text{det} \bigg\vert \frac{\partial f^{-1}}{\partial \mathbf{z}^\prime} \bigg\vert = q(\mathbf{z}) \text{det} \bigg\vert \frac{\partial f}{\partial \mathbf{z}} \bigg\vert^{-1}.
\end{equation}
\newline

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{images/graphical_models/normalizing_flows.png}
    \caption{Normalizing flows applied to a 2D isotropic Gaussian, using transformations of the form given in eq. \ref{eq: normalizing flows transformation}. Increasing the flow length $K$ allows for more flexible approximate posterior distributions. Reproduced from \cite{rezende2015variational}.}
    \label{fig: normalizing flows}
\end{figure}

\noindent Note that the mapping, $f$, can also depend on other inputs, such as observed variables $\mathbf{x}$ or other auxiliary inputs. By composing multiple mappings, we can construct arbitrarily complex densities. Thus, starting from an initial distribution of $q_0 (\mathbf{z}_0)$, a flow of length $K$ can be computed as

\begin{equation}
	\log q_K (\mathbf{z}_K) = \log q_0 (\mathbf{z}_0) - \sum_{k=1}^K \log \text{det}  \bigg\vert \frac{\partial f_k}{\partial \mathbf{z}_{k-1}} \bigg\vert.
\end{equation}

\noindent This sequence can be interpreted as performing expansions and contractions on the initial density to construct a more flexible form. 

To parameterize the mappings, \cite{rezende2015variational} use transformations of the form

\begin{equation}
	f(\mathbf{z}) = \mathbf{z} + \mathbf{u}h(\mathbf{w}^\intercal \mathbf{z} + b)
	\label{eq: normalizing flows transformation}
\end{equation}

\noindent where $\mathbf{u}$, $\mathbf{w}$, and $b$ are learned parameters and $h(\cdot)$ is a non-linear function. This class of normalizing flows is referred to as $planar flows$. The determinant of the Jacobian of this transformation is

\begin{equation}
	\text{det} \bigg\vert  \frac{\partial f}{\partial \mathbf{z}} \bigg\vert = \big\vert \text{det} \left( \mathbf{I} + \mathbf{u} (h^\prime (\mathbf{w}^\intercal \mathbf{z} + b) \mathbf{w})^\intercal \right) \big\vert = \big\vert 1 + \mathbf{u}^\intercal h^\prime (\mathbf{w}^\intercal \mathbf{z} + b) \mathbf{w} \big\vert
\end{equation}

\noindent Finally, putting everything together, we can use normalizing flows to draw samples from the final, more flexible approximate posterior, $q(\mathbf{z} | \mathbf{x}) = q_K (\mathbf{z}_K)$. The variational lower bound becomes

\begin{equation}
	\mathcal{L} = \mathbb{E}_{\mathbf{z} \sim q(\mathbf{z}, \mathbf{x})} \left[ \log p(\mathbf{x}, \mathbf{z}) - \log q(\mathbf{z}|\mathbf{x}) \right]
	\label{eq: norm flows lower bound 1}
\end{equation}

\begin{equation}
	\mathcal{L} = \mathbb{E}_{\mathbf{z}_0 \sim q(\mathbf{z}_0, \mathbf{x})} \left[ \log p(\mathbf{x}, \mathbf{z}_K) - \log q(\mathbf{z}_0|\mathbf{x}) + \sum_{k=1}^K \log \big\vert 1 + \mathbf{u}_k^\intercal h_k^\prime (\mathbf{w}_k^\intercal \mathbf{z}_{k-1} + b_k) \mathbf{w}_k \big\vert \right]
	\label{eq: norm flows lower bound 2}
\end{equation}

\noindent Note that in going to eq. \ref{eq: norm flows lower bound 1} to eq. \ref{eq: norm flows lower bound 2} we have used the \textit{law of the unconscious statistician (LOTUS)}, which allows one to take expectations of a transformed variable (or any function thereof) using the original variable. If $\mathbf{z}^\prime = f(\mathbf{z})$ and $g$ is some arbitrary function of $\mathbf{z}^\prime$, then this can be expressed as

\begin{equation}
	\mathbb{E}_{\mathbf{z}^\prime \sim q (\mathbf{z}^\prime)} \left[ g(\mathbf{z}^\prime) \right] = \mathbb{E}_{\mathbf{z} \sim q (\mathbf{z})} \left[ g( f (\mathbf{z})) \right].
\end{equation}

\noindent Thus, we can take expectations with respect to the final density, i.e. the approximate posterior, using samples drawn from the initial density. In an amortized inference setting, the inference model can output the parameters of the initial distribution as well as parameters for each stage of the flow. The flow parameters could also be learned as global parameters.


Another class of normalizing flows is \textit{inverse auto-regressive flow (IAF)} \cite{kingma2016improved}. These transformations take inspiration from auto-regressive models, which factor joint probability distributions into a series of conditional distributions using the chain rule of probability.


\section{Filtering Variational Inference}
\label{sec: variational inference in dynamical models}

We assume the dynamic directed latent variable model set-up given in Section \ref{sec: dynamical latent variable models}. We can bound the marginal log-likelihood of the observation sequence by introducing an approximate posterior distribution, $q(\mathbf{z}_{1:T} | \mathbf{x}_{1:T})$, then using Jensen's inequality to write
\begin{equation}
    \log p(\mathbf{x}_{1:T}) \geq \mathbb{E}_{q(\mathbf{z}_{1:T} | \mathbf{x}_{1:T})} \left[ \log \frac{p(\mathbf{x}_{1:T} , \mathbf{z}_{1:T})}{q(\mathbf{z}_{1:T} | \mathbf{x}_{1:T})} \right] \equiv - \mathcal{F}_E,
    \label{eq: vi_1}
\end{equation}
where $\mathcal{F}_E$ is the total \textit{variational free-energy}. For now, we assume the generative model takes the form given by eq. \ref{eq: general dynamics model}, and the approximate posterior takes the form:
\begin{equation}
    q(\mathbf{z}_{1:T} | \mathbf{x}_{1:T}) = \prod_{t=1}^T q(\mathbf{z}_t | \mathbf{x}_{1:t} , \mathbf{z}_{1:t-1}).
    \label{eq: approx post form}
\end{equation}
In other words, we assume we are in the online setting, and therefore use a \textit{filtering} approximate posterior that does not condition on future observations or latent variables. Using eqs. \ref{eq: general dynamics model} and \ref{eq: approx post form}, we can rewrite eq. \ref{eq: vi_1} as
\begin{equation}
    \log p(\mathbf{x}_{1:T}) \geq \mathbb{E}_{q(\mathbf{z}_{1:T} | \mathbf{x}_{1:T})} \left[ \sum_{t=1}^T \log \frac{p(\mathbf{x}_t | \mathbf{x}_{1:t-1} , \mathbf{z}_{1:t}) p(\mathbf{z}_t | \mathbf{x}_{1:t-1} , \mathbf{z}_{1:t-1})}{q(\mathbf{z}_t | \mathbf{x}_{1:t} , \mathbf{z}_{1:t-1})} \right].
\end{equation}
Defining the term
\begin{equation}
    C_t \equiv \log \frac{p(\mathbf{x}_t | \mathbf{x}_{1:t-1} , \mathbf{z}_{1:t}) p(\mathbf{z}_t | \mathbf{x}_{1:t-1} , \mathbf{z}_{1:t-1})}{q(\mathbf{z}_t | \mathbf{x}_{1:t} , \mathbf{z}_{1:t-1})},
\end{equation}
then
\begin{align}
    \log p(\mathbf{x}_{1:T}) & \geq \mathbb{E}_{q(\mathbf{z}_{1:T} | \mathbf{x}_{1:T})} \left[ \sum_{t=1}^T C_t \right] \\
    & = \mathbb{E}_{q(\mathbf{z}_1 | \mathbf{x}_1)} \mathbb{E}_{q(\mathbf{z}_2 | \mathbf{x}_{1:2} , \mathbf{z}_1)} \dots \mathbb{E}_{q(\mathbf{z}_T | \mathbf{x}_{1:T} , \mathbf{z}_{1:T-1})} \left[ \sum_{t=1}^T C_t \right].
\end{align}
% \begin{align}
%     \log p(\mathbf{x}_{1:T}) & \geq \mathbb{E}_{q(\mathbf{z}_{1:T} | \mathbf{x}_{1:T})} \left[ \sum_{t=1}^T C_t \right] \\
%     & = \int_{\mathbf{z}_1} q(\mathbf{z}_1 | \mathbf{x}_1) \int_{\mathbf{z}_2} q(\mathbf{z}_2 | \mathbf{z}_1 , \mathbf{x}_{1:2}) \int_{\mathbf{z}_3} \dots \int_{\mathbf{z}_T} q(\mathbf{z}_T | \mathbf{z}_{1:T-1} , \mathbf{x}_{1:T}) \sum_{t=1}^T C_t
% \end{align}
There are $T$ terms within the sum, but because we do not condition on future variables, each $C_t$ only depends on the expectations up to time $t$. This allows us to write:
\begin{align}
    \log p(\mathbf{x}_{1:T}) & \geq \mathbb{E}_{q(\mathbf{z}_1 | \mathbf{x}_1)} \left[ C_1 \right] \nonumber \\ 
    & + \mathbb{E}_{q(\mathbf{z}_1 | \mathbf{x}_1)} \mathbb{E}_{q(\mathbf{z}_2 | \mathbf{z}_1 , \mathbf{x}_{1:2})} \left[ C_2 \right] \nonumber \\
    & + \dots \nonumber \\
    & + \mathbb{E}_{q(\mathbf{z}_1 | \mathbf{x}_1)} \mathbb{E}_{q(\mathbf{z}_2 | \mathbf{z}_1 , \mathbf{x}_{1:2})} \dots \mathbb{E}_{q(\mathbf{z}_T | \mathbf{z}_{1:T-1} , \mathbf{x}_{1:T})} \left[ C_T \right] \\
    & = \sum_{t=1}^T \mathbb{E}_{q(\mathbf{z}_{1:t} | \mathbf{x}_{1:t})} \left[ C_t \right] \\
    & = \sum_{t=1}^T \mathbb{E}_{\prod_{\tau=1}^t q(\mathbf{z}_\tau | \mathbf{x}_{1:\tau} , \mathbf{z}_{1:\tau-1})} \left[ C_t \right] \\
    & = \sum_{t=1}^T \mathbb{E}_{\prod_{\tau=1}^{t-1} q(\mathbf{z}_\tau | \mathbf{x}_{1:\tau} , \mathbf{z}_{1:\tau-1})} \left[ \mathbb{E}_{q(\mathbf{z}_t | \mathbf{x}_{1:t} , \mathbf{z}_{1:t-1})} \left[ C_t \right] \right]
\end{align}
% \begin{align}
%     \log p(\mathbf{x}_{1:T}) & \geq \int_{\mathbf{z}_1} q(\mathbf{z}_1 | \mathbf{x}_1) C_1 \\ 
%     & + \int_{\mathbf{z}_1} q(\mathbf{z}_1 | \mathbf{x}_1) \int_{\mathbf{z}_2} q(\mathbf{z}_2 | \mathbf{z}_1 , \mathbf{x}_{1:2}) C_2 \\
%     & + \dots \\
%     & + \int_{\mathbf{z}_1} q(\mathbf{z}_1 | \mathbf{x}_1) \int_{\mathbf{z}_2} q(\mathbf{z}_2 | \mathbf{z}_1 , \mathbf{x}_{1:2}) \dots \int_{\mathbf{z}_T} q(\mathbf{z}_T | \mathbf{z}_{1:T-1} , \mathbf{x}_{1:T}) C_T \\
%     & = \sum_{t=1}^T \mathbb{E}_{q(\mathbf{z}_{1:t} | \mathbf{x}_{1:t})} \left[ C_t \right] \\
%     & = \sum_{t=1}^T \mathbb{E}_{\prod_{\tau=1}^t q(\mathbf{z}_\tau | \mathbf{x}_{1:\tau} , \mathbf{z}_{1:\tau-1})} \left[ C_t \right] \\
%     & = \sum_{t=1}^T \mathbb{E}_{\prod_{\tau=1}^{t-1} q(\mathbf{z}_\tau | \mathbf{x}_{1:\tau} , \mathbf{z}_{1:\tau-1})} \left[ \mathbb{E}_{q(\mathbf{z}_t | \mathbf{x}_{1:t} , \mathbf{x}_{1:t})} \left[ C_t \right] \right] \\
% \end{align}
The total variational free-energy is thus the sum of per-time-step variational free-energies, with expectations taken w.r.t. all past latent variables:
\begin{equation}
    \mathcal{F}_E = - \sum_{t=1}^T \mathbb{E}_{\prod_{\tau=1}^{t-1} q(\mathbf{z}_\tau | \mathbf{x}_{1:\tau} , \mathbf{z}_{1:\tau-1})} \left[ \mathbb{E}_{q(\mathbf{z}_t | \mathbf{x}_{1:t} , \mathbf{z}_{1:t-1})} \left[ C_t \right] \right].
\end{equation}

Evaluating these outer expectations becomes computationally intractable as the sequence length grows. When approximating each of the $T$ expectations with $m$ Monte Carlo samples, evaluating the summation requires drawing $\mathcal{O}(m^T)$ samples. For $m > 1$, the number of samples, and therefore the amount of computation, blows up as $T \rightarrow \infty$. Thus, to retain tractability, we can set $m=1$ for expectations over all past time steps, thereby evaluating the path summation of the per time step variational free-energies, defined as the \textit{variational free-action}, $\mathcal{F}_A$:
\begin{equation}
    \mathcal{F}_A \equiv - \sum_{t=1}^T \mathbb{E}_{q(\mathbf{z}_t | \mathbf{x}_{1:t} , \mathbf{z}_{1:t-1})} \left[ C_t \right] \bigg \vert_{\mathbf{z}_{1:t-1}}
\end{equation}
where $\mathbf{z}_{1:t-1}$ is the sampled trajectory of the past latent variables. The free-action evaluates a single path through the latent space, whereas the free-energy evaluates all possible paths. However, the free-action still provides a lower bound on the marginal log-likelihood of the sequence. Need to show this...
